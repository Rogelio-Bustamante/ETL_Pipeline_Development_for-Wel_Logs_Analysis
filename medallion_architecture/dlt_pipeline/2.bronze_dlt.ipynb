{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "842e7fae-7d0d-4c30-bd57-b02f215221c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Raneme and dedup columns (bronze to silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b9d218-9346-4591-bbb0-ef77baa3d2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bcb3760-e076-49c9-831f-8d8cb47d36fd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753909348505}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#@dlt.expect_or_drop(\"no_duplicates\", \"ROW_NUMBER() OVER (PARTITION BY measurement_time ORDER BY well_id) = 1\")\n",
    "@dlt.table(\n",
    "    name = \"all_wells_silver\",\n",
    "    comment = \"Table that contains all wells in silver layer\",\n",
    "    table_properties = {\"layer\" : \"silver\", \"type\" : \"well log\"}\n",
    ")\n",
    "\n",
    "def all_wells_silver():\n",
    "  df = dlt.read_stream(\"all_wells_bronze\")\n",
    "\n",
    "  ## To rename duplicated columns and merge columns pointing to the same target name (i.e. deduplicate)\n",
    "  column_map = {\n",
    "      \"DEPT\": \"DEPTH\", \"Gamma\": \"GR\",\n",
    "      \"RILD\": \"ILD\", \"RILM\": \"ILM\"\n",
    "  }\n",
    "\n",
    "  # Merge columns pointing to the same target name\n",
    "  renamed_cols = []\n",
    "  already_mapped = set()\n",
    "  df_cols = df.schema.names\n",
    "  \n",
    "  for src_col, dest_col in column_map.items():\n",
    "      if dest_col not in already_mapped:\n",
    "          # Find all the columns that maps the destination name\n",
    "          cols_to_merge = [col(c) for c, d in column_map.items() if d == dest_col and c in df_cols]\n",
    "          # If the destination name already existed as the original column, we also include it\n",
    "          if dest_col in df_cols:\n",
    "              cols_to_merge.append(col(dest_col))\n",
    "          # Create final column by coalesce (fist value priority)\n",
    "          if cols_to_merge:\n",
    "              renamed_cols.append(coalesce(*cols_to_merge).alias(dest_col))\n",
    "              already_mapped.add(dest_col)\n",
    "  \n",
    "  # Add columns unmapped and that in the colisionan\n",
    "  for c in df_cols:\n",
    "      if c not in column_map and c not in already_mapped:\n",
    "          renamed_cols.append(col(c))\n",
    "  \n",
    "  # Final DataFrame\n",
    "  df_dedup = df.select(renamed_cols)\n",
    "  df_dedup_cols = df_dedup.columns\n",
    "\n",
    "  # Rename columns conditionally and use the same standard for each log\n",
    "  column_names = [col(\"WELL_NAME\").alias(\"well_id\")]\n",
    "  column_names.extend(col(c) for c in df_dedup_cols if c not in (\"WELL_NAME\",\"RECORDED_IN\"))\n",
    "  column_names.append(col(\"RECORDED_IN\").alias(\"measurement_time\"))\n",
    "  \n",
    "  return (\n",
    "      df_dedup.select(column_names)\n",
    "      .withColumn(\"measurement_time\", to_timestamp(\"measurement_time\"))\n",
    "      .withColumn(\"silver_ingestion_time\", current_timestamp())\n",
    "      .drop(\"bronze_ingestion_time\", \"_rescued_data\", \"file_mod_time\")\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5670541534942516,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.bronze_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
