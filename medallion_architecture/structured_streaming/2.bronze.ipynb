{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b9d218-9346-4591-bbb0-ef77baa3d2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bcb3760-e076-49c9-831f-8d8cb47d36fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoints_path = \"/Volumes/well_logs_nrt/autoloader/checkpoints_volume/all_wells_silver_checkpoints\"\n",
    "output_path = \"/Volumes/well_logs_nrt/silver/deltalake_folders_volume/all_wells_silver\"\n",
    "input_path = \"/Volumes/well_logs_nrt/bronze/deltalake_folders_volume/all_wells_bronze\"\n",
    "\n",
    "df = spark.readStream.format(\"delta\").load(input_path)\n",
    "\n",
    "## This part is to rename duplicated columns and merge columns pointing to the same target name (i.e. deduplicate)\n",
    "\n",
    "column_map = {\n",
    "    \"DEPT\": \"DEPTH\", \"Gamma\": \"GR\",\n",
    "    \"RILD\": \"ILD\", \"RILM\": \"ILM\"\n",
    "}\n",
    "\n",
    "# Merge columns pointing to the same target name\n",
    "renamed_cols = []\n",
    "already_mapped = set()\n",
    "df_cols = df.columns # This is to avoid accessing several times and involve re-reading the scheme. In this case df.columns works because we are working on an interactive notebook, out of DLT, and the schema can be inferred at run time.\n",
    "\n",
    "for src_col, dest_col in column_map.items():\n",
    "    if dest_col not in already_mapped:\n",
    "        # Find all the columns that map to this destination name\n",
    "        cols_to_merge = [col(c) for c, d in column_map.items() if d == dest_col and c in df_cols]\n",
    "        # If the destination name already existed as the original column, we also include it\n",
    "        if dest_col in  df_cols:\n",
    "            cols_to_merge.append(col(dest_col))\n",
    "        # Create final column by coalesce (fist value priority)\n",
    "        if cols_to_merge:\n",
    "            #This expression utilizes the coalesce function to return the first non-null value from a list of columns. The *cols_to_merge part unpacks a list or tuple of column names or Column objects, passing them as individual arguments to the coalesce function. \n",
    "            renamed_cols.append(coalesce(*cols_to_merge).alias(dest_col))\n",
    "            already_mapped.add(dest_col)\n",
    "\n",
    "# Add columns unmapped and that in the colisionan\n",
    "for c in df_cols:\n",
    "    if c not in column_map and c not in already_mapped:\n",
    "        renamed_cols.append(col(c))\n",
    "print(renamed_cols)\n",
    "\n",
    "# Final DataFrame\n",
    "df_dedup = df.select(renamed_cols)\n",
    "df_dedup_cols = df_dedup.columns\n",
    "\n",
    "# Rename columns conditionally and use the same standard for each log\n",
    "\n",
    "column_names = [col(\"WELL_NAME\").alias(\"well_id\")]\n",
    "column_names.extend(col(c) for c in df_dedup_cols if c not in (\"WELL_NAME\",\"RECORDED_IN\"))\n",
    "# column_names += [col(c) for c in df_dedup_cols if c not in (\"WELL_NAME\",\"RECORDED_IN\")]\n",
    "column_names.append(col(\"RECORDED_IN\").alias(\"measurement_time\"))\n",
    "df_renamed = (df_dedup.select(column_names)\n",
    "             .withColumn(\"measurement_time\", to_timestamp(\"measurement_time\"))\n",
    "             .withColumn(\"silver_ingestion_time\", current_timestamp())\n",
    "             .drop(\"bronze_ingestion_time\", \"_rescued_data\", \"file_mod_time\")\n",
    ")\n",
    "\n",
    "df_renamed.writeStream.format(\"delta\") \\\n",
    "  .option(\"checkpointLocation\", checkpoints_path) \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .trigger(once=True) \\\n",
    "  .start(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65369725-602f-49e4-ad8b-f8f746d03e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac4f77e-172b-4e33-aad4-4eeb51f0b4ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754521658383}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format(\"delta\").load(\"/Volumes/well_logs_nrt/silver/deltalake_folders_volume/all_wells_silver\")\n",
    "        .where(\"well_id = 'F02-01_logs'\")\n",
    "        #.orderBy(col(\"well_id\").asc(), col(\"depth\").asc())\n",
    "        .orderBy(\"well_id\", \"depth\", ascending = [True, True])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5670541534942516,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
